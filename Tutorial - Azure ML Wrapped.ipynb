{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e1aec0",
   "metadata": {},
   "source": [
    "## Preliminary steps\n",
    "1. Make sure to change the variables specified whenever there is a \"TODO:\"\n",
    "2. When uploading the data make sure to select the type as \"File (url_file)\"\n",
    "3. The file name could be different but make sure to change accordingly.\n",
    "![Uploading_dataset](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/Upload_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f5be3",
   "metadata": {},
   "source": [
    "4. Change the kernel on the top right corner to be from Python 3 (ipykernel) to be Python 3.8 - Azure ML\n",
    "![Select_kernel.png](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/Select_kernel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b502b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd002bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install Azure SDK library\n",
    "!pip3 install azure-ai-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa5a6c",
   "metadata": {},
   "source": [
    "## Connect to the workspace\n",
    "\n",
    "Before you dive in the code, you'll need to connect to your Azure ML workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
    "\n",
    "We're using `DefaultAzureCredential` to get access to workspace. \n",
    "`DefaultAzureCredential` is used to handle most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it doesn't work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle to the workspace\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9bc45",
   "metadata": {},
   "source": [
    "In the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find these values:\n",
    "\n",
    "1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n",
    "1. Copy the value for workspace, resource group and subscription ID into the code.  \n",
    "1. You'll need to copy one value, close the area and paste, then come back for the next one.\n",
    "\n",
    "![Credentials.png](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/Credentials.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: change parameters below\n",
    "# Get a handle to the workspace\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"xxxxxx\",\n",
    "    resource_group_name=\"xxxxx\",\n",
    "    workspace_name=\"xxxxx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43809c76",
   "metadata": {},
   "source": [
    "## Create a compute resource to run your job\n",
    "\n",
    "You already have a compute instance you're using to run the notebook.  But now you'll add another type, a **compute cluster** that you'll use to run your training job. The compute cluster can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n",
    "\n",
    "You'll provision a Linux compute cluster. See the [full list on VM sizes and prices](https://azure.microsoft.com/pricing/details/machine-learning/) .\n",
    "\n",
    "For this example, you only need a basic cluster, so you'll use a Standard_DS11_v2 model with 2 vCPU cores, 14-GB RAM. If you have already created a compute cluster, please specify its name in **cpu_compute_target** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf38750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# Name assigned to the compute cluster\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure ML compute object with the intended parameters\n",
    "    cpu_cluster = AmlCompute(\n",
    "        name=cpu_compute_target,\n",
    "        # Azure ML Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"STANDARD_DS11_V2\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=1,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "    print(\n",
    "         f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
    "          )\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eaf66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "612d4d6c",
   "metadata": {},
   "source": [
    "## Create a job environment\n",
    "\n",
    "To run your AzureML job on your compute cluster, you'll need an [environment](https://docs.microsoft.com/azure/machine-learning/concept-environments). An environment lists the software runtime and libraries that you want installed on the compute where youâ€™ll be training. It's similar to your Python environment on your local machine.\n",
    "\n",
    "AzureML provides many curated or ready-made environments, which are useful for common training and inference scenarios. You can also create your own custom environments using a docker image, or a conda configuration.\n",
    "\n",
    "In this example, you'll create a custom conda environment for your jobs, using a conda yaml file.\n",
    "\n",
    "First, create a directory to store the file in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645cfe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36a3b1",
   "metadata": {},
   "source": [
    "Now, create the file in the dependencies directory. The cell below uses IPython magic to write the conda.yml file into the directory you just created.\\\n",
    "For dependecies, we need to list Python version and all library versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa7627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - seaborn=0.12.2 \n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 1.26.1\n",
    "    - azureml-mlflow==1.42.0\n",
    "    - psutil>=5.8,<5.9\n",
    "    - tqdm>=4.59,<4.60\n",
    "    - ipykernel~=6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82188234",
   "metadata": {},
   "source": [
    "**Note:** The reason why numpy, scikit-learn, scipy, pandas, and seaborn are not listed under pip section in the conda.yml file is that these packages are available in the Conda package manager and can be installed using the conda command.\n",
    "\n",
    "Conda is a package manager that can install packages from different channels, including conda-forge, which is specified in the channels section of the conda.yml file. By specifying these packages under the dependencies section, Conda will ensure that the specified versions of these packages, along with their dependencies, are installed in the environment. This ensures compatibility and stability of the environment.\n",
    "\n",
    "On the other hand, pip is a package manager for Python packages that are not available in the Conda channels. The pip section in the conda.yml file is used to specify additional Python packages that are not available in Conda channels and need to be installed using pip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d11c2",
   "metadata": {},
   "source": [
    "\n",
    "Reference this *yaml* file to create and register this custom environment in your workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-scikit-learn\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for Bank Deposit pipeline\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175cd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ef12772",
   "metadata": {},
   "source": [
    "## What is a command job?\n",
    "\n",
    "You'll create an Azure ML *command job* to train a model for credit default prediction. The command job is used to run a *training script* in a specified environment on a specified compute resource.  You've already created the environment and the compute resource.  Next you'll create the training script.\n",
    "\n",
    "The *training script* handles the data preparation, training and registering of the trained model. In this tutorial, you'll create a Python training script.\n",
    "\n",
    "Command jobs can be run from CLI, Python SDK, or studio interface. In this tutorial, you'll use the Azure ML Python SDK v2 to create and run the command job.\n",
    "\n",
    "After running the training job, you'll deploy the model, then use it to produce a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde9aa0",
   "metadata": {},
   "source": [
    "### Build the command job to train\n",
    "Now that you have all assets required to run your job, it's time to build the job itself, using the Azure ML Python SDK v2. We will be creating a command job.\n",
    "\n",
    "An AzureML command job is a resource that specifies all the details needed to execute your training code in the cloud: inputs and outputs, the type of hardware to use, software to install, and how to run your code. the command job contains information to execute a single command.\n",
    "\n",
    "**Create training script**\n",
    "\n",
    "Let's start by creating the training script - the *main.py* python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e17638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_src_dir = \"./src\"\n",
    "os.makedirs(train_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8abb45",
   "metadata": {},
   "source": [
    "This script handles the preprocessing of the data, splitting it into test and train data. It then consumes this data to train a Random forest model and return the output model. It is essentially the finalized and clean version of **Tutorial - Predictive Model.ipynb**, which focuses on constructing the final model.\n",
    "\n",
    "[MLFlow](https://mlflow.org/docs/latest/tracking.html) will be used to log the parameters and metrics during our pipeline run. \n",
    "\n",
    "The cell below uses IPython magic to write the training script into the directory you just created. Additionally, you could create the main.py using any text editor programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb257b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
    "    parser.add_argument(\"--max_depth\", required=False, default=15, type=float)\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    args = parser.parse_args()\n",
    "   \n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # enable autologging\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    ###################\n",
    "    # <Load the data>\n",
    "    ###################\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.data)\n",
    "    \n",
    "    df_bank = pd.read_csv(args.data)\n",
    "\n",
    "    # Log the size of dataframe\n",
    "    mlflow.log_metric(\"num_samples\", df_bank.shape[0])\n",
    "    mlflow.log_metric(\"num_features\", df_bank.shape[1] - 1)\n",
    "    \n",
    "    ###################\n",
    "    # </Load the data>\n",
    "    ###################\n",
    "    \n",
    "    ##################\n",
    "    #<Data preprocessing>\n",
    "    ##################\n",
    "    \n",
    "    # Copying original dataframe\n",
    "    df_bank_ready = df_bank.copy()\n",
    "    \n",
    "    # Select Features\n",
    "    feature = df_bank.drop('deposit', axis=1)\n",
    "\n",
    "    # Select Target\n",
    "    target = df_bank['deposit'].apply(lambda deposit: 1 if deposit == 'yes' else 0)\n",
    "\n",
    "    # Set Training and Testing Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature , target, \n",
    "                                                        shuffle = True, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=1)\n",
    "\n",
    "    # Transform data\n",
    "    numeric_columns = ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']\n",
    "    categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "    # define a function that cleans the balance column\n",
    "    def clean_balance(x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    # define a custom transformer that applies the clean_balance function to the balance column\n",
    "    clean_balance_transformer = FunctionTransformer(clean_balance)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "    # Embeded both transformation into ColumnTransformer so that it could automatically transform data when \n",
    "    # having new data\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('clean_balance', clean_balance_transformer, ['balance']),\n",
    "        ('num', scaler, numeric_columns),\n",
    "        ('cat', one_hot_encoder, categorical_columns)\n",
    "    ])\n",
    "\n",
    "    # We fit preprocessor with X_train instead of the whole dataset to prevent data leakage\n",
    "    preprocessor.fit(X_train)\n",
    "    \n",
    "    X_train_preprocessed = preprocessor.transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "    print(f\"Training with data of shape {X_train_preprocessed.shape}\")\n",
    "    \n",
    "    ##################\n",
    "    #</Data preprocessing>\n",
    "    ##################\n",
    "\n",
    "    ##################\n",
    "    #<train the model>\n",
    "    ##################\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=args.n_estimators, max_depth=args.max_depth,\n",
    "        min_samples_split=40, min_samples_leaf=60\n",
    "    )\n",
    "    clf.fit(X_train_preprocessed, y_train)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', clf)\n",
    "    ])\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "#     y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    ###################\n",
    "    #</train the model>\n",
    "    ###################\n",
    "\n",
    "    ##########################\n",
    "    #<save and register model>\n",
    "    ##########################\n",
    "    # Registering the model to the workspace\n",
    "    print(\"Registering the model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=pipeline,\n",
    "        path=os.path.join(args.registered_model_name, \"trained_model\"),\n",
    "    )\n",
    "    ###########################\n",
    "    #</save and register model>\n",
    "    ###########################\n",
    "    \n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a67c3",
   "metadata": {},
   "source": [
    "As you can see in this script, once the model is trained, the model file is saved and registered to the workspace. Now you can use the registered model in inferencing endpoints.\n",
    "\n",
    "## Configure the command\n",
    "\n",
    "Now that you have a script that can perform the desired tasks, you'll use the general purpose **command** that can run command line actions. This command line action can be directly calling system commands or by running a script. \n",
    "\n",
    "Here, you'll create input variables to specify the input data, split ratio, learning rate and registered model name.  The command script will:\n",
    "* Use the compute created earlier to run this command.\n",
    "* Use the environment created earlier - you can use the `@latest` notation to indicate the latest version of the environment when the command is run.\n",
    "* Configure some metadata like display name, experiment name etc. An *experiment* is a container for all the iterations you do on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure ML studio.\n",
    "* Configure the command line action itself - `python main.py` in this case. The inputs/outputs are accessible in the command via the `${{ ... }}` notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81110ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO:\n",
    "\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "registered_model_name = \"deposit-prediction-model\"\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        data=Input(\n",
    "            type=\"uri_file\",\n",
    "            # TODO: Change the path variable to be your dataset name\n",
    "            # \"path\" is a reference to a dataset named \"bank-dataset\" in the Azure Machine Learning workspace, version 1.\n",
    "            path=\"azureml:bank-dataset:1\",\n",
    "        ),\n",
    "        test_train_ratio=0.2,\n",
    "        # Specify hyperparameters of Random Forest\n",
    "        max_depth=10,\n",
    "        n_estimators=300,\n",
    "        registered_model_name=registered_model_name,\n",
    "    ),\n",
    "    code=\"./src/\",  # location of source code\n",
    "    command=\"python main.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --max_depth ${{inputs.max_depth}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
    "    environment=\"aml-scikit-learn@latest\",\n",
    "    compute=\"cpu-cluster\",\n",
    "    experiment_name=\"train_model_deposit_prediction\",\n",
    "    display_name=\"deposit-prediction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e714c",
   "metadata": {},
   "source": [
    "## Submit the job \n",
    "\n",
    "It's now time to submit the job to run in AzureML. This time you'll use `create_or_update`  on `ml_client.jobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dfb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2fcc3",
   "metadata": {},
   "source": [
    "## View job output and wait for job completion\n",
    "\n",
    "View the job in Azure ML studio by selecting the link in the output of the previous cell. \n",
    "\n",
    "The output of this job will look like this in Azure ML studio. Explore the tabs for various details like metrics, outputs etc. Once completed, the job will register a model in your workspace as a result of training. \n",
    "\n",
    "![Screenshot that shows the job overview](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/view-job.gif \"View the job in studio\")\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Wait until the status of the job is complete before returning to this notebook to continue. The job will take 2 to 3 minutes to run. It could take longer (up to 10 minutes) if the compute cluster has been scaled down to zero nodes and custom environment is still building.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e39c90",
   "metadata": {},
   "source": [
    "To see the logs and examine errors if any, click on **Outputs + logs** tab\n",
    "![Logs](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/Outputs+logs.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1153f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca0183e",
   "metadata": {},
   "source": [
    "# Deploy model\n",
    "Once the job is completed, you would see a model in the **Models** tab\n",
    "![Logs](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/Models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85135fc7",
   "metadata": {},
   "source": [
    "Click on deposit-prediction-model, Deploy tab and select Real-time endpoint\n",
    "![Logs](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/Realtime_endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60c550",
   "metadata": {},
   "source": [
    "Use the following configuration, but keep in mind that the **Endpoint name** and **Deployment name** are not required to be the same as shown in the screenshot. This deployment requires a virtual machine to host the endpoint which would use a specified environment in conda.yml and Python script main.py.\n",
    "![Endpoint_configuration](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/Endpoint_configuration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a6dce",
   "metadata": {},
   "source": [
    "Make sure to remove this endpoint once you are finished using it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f240345",
   "metadata": {},
   "source": [
    "## Test endpoint\n",
    "To test the endpoint, go to Endpoints tab and click on the created endpoint.\n",
    "![Logs](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/Endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edde89c",
   "metadata": {},
   "source": [
    "Click the Test tab, enter the following JSON schema, and press the Test button.\n",
    "![Test_endpoint](https://raw.githubusercontent.com/Khaninsi/Azure-MLOps/master/screenshots/Test_endpoint.png)\n",
    "If no error messages appear, the deployment was successful and completed. Congrats!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d81a7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cac1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e4dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
